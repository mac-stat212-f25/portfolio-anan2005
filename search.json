[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "COMP/STAT212 Portfolio",
    "section": "",
    "text": "Welcome\nWelcome to my online portfolio for COMP/STAT112 course taken at Macalester College. Please, use the side bar on the left for navigation.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "src/pv/pv-01.html",
    "href": "src/pv/pv-01.html",
    "title": "Professional Viz Sample",
    "section": "",
    "text": "Add content here",
    "crumbs": [
      "Prof Viz",
      "<span class='chapter-number'>1</span>¬† <span class='chapter-title'>Professional Viz Sample</span>"
    ]
  },
  {
    "objectID": "src/tt/hw01-tt.html",
    "href": "src/tt/hw01-tt.html",
    "title": "Homework 01",
    "section": "",
    "text": "TidyTuesday Section\nExplore the week‚Äôs TidyTuesday challenge. Develop a research question, then answer it through a short data story with effective visualization(s). Provide sufficient background for readers to grasp your narrative.\nCodelibrary(ggplot2)\nCodefrogID_data &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-02/frogID_data.csv')\nfrog_names &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-02/frog_names.csv')\nCode# Daily recording counts\n\nggplot(frogID_data, aes(x = eventDate)) +\n  geom_histogram(fill = \"steelblue\") +\n  labs(title = \"Daily Frog Records in 2023\",\n    x = \"Date\", y = \"Number of Records\") +\n  theme_minimal()\nCode# Record time of the day\n\nggplot(frogID_data, aes(x = eventTime)) +\n  geom_histogram(fill = \"steelblue\") +\n  labs(title = \"Recordings by time of Day\",\n    x = \"Hour\", y = \"Count\") +\n  theme_minimal()\nCode# Frog location distribution map\n\nlibrary(leaflet)\nleaflet(frogID_data) %&gt;% \n  addTiles() %&gt;% \n  addCircleMarkers(~decimalLongitude, ~decimalLatitude,\n                   radius = 4, color = \"darkgreen\")",
    "crumbs": [
      "TidyTuesday",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Homework 01</span>"
    ]
  },
  {
    "objectID": "src/tt/hw01-tt.html#tidytuesday-section",
    "href": "src/tt/hw01-tt.html#tidytuesday-section",
    "title": "Homework 01",
    "section": "",
    "text": "Research question: How do the temporal and geographical factors affect the record of frogs in Australia in 2023?\n\n\n\n\n\nConclusion: Frogs tend to occur more often around October, which is the spring season in Australia. It reflects frogs prefer warmer weather than colder one. According to the histogram graph that depicts the time frogs being recorded in the day, they are most active around 10am in the morning and 20pm at night. Based on the point map, we can clearly perceive that frogs like to gather near the water source.",
    "crumbs": [
      "TidyTuesday",
      "<span class='chapter-number'>2</span>¬† <span class='chapter-title'>Homework 01</span>"
    ]
  },
  {
    "objectID": "src/tt/hw02-tt.html",
    "href": "src/tt/hw02-tt.html",
    "title": "Homework 02",
    "section": "",
    "text": "TidyTuesday Section (optional)\nExplore the week‚Äôs TidyTuesday challenge. Develop a research question, then answer it through a short data story with effective visualization(s). Provide sufficient background for readers to grasp your narrative.\nCodecountry_lists &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-09/country_lists.csv')\nrank_by_year &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-09/rank_by_year.csv')\nCodelibrary(tidyverse)\nCoderegion_avg &lt;- rank_by_year |&gt; \n  group_by(region, year) |&gt; \n  summarize(avg_free_visa = mean(visa_free_count, na.rm = TRUE))\n\nggplot(region_avg, aes(x = year, y = avg_free_visa, color = region)) +\n  geom_line(size = 1) +\n  geom_point(size = 2) +\n  scale_x_continuous(breaks = seq(2006, 2025, by = 2)) +\n  labs(title = \"Visa-Free Access by Region (2006‚Äì2025)\",\n       subtitle = \"Average number of visa-free destinations per passport\",\n       x = \"Year\",\n       y = \"Average Visa-Free Count\",\n       color = \"Region\") +\n  theme_minimal()",
    "crumbs": [
      "TidyTuesday",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Homework 02</span>"
    ]
  },
  {
    "objectID": "src/tt/hw02-tt.html#tidytuesday-section-optional",
    "href": "src/tt/hw02-tt.html#tidytuesday-section-optional",
    "title": "Homework 02",
    "section": "",
    "text": "Instructions\n\n\n\nYou can count work on this week‚Äôs TidyTuesday toward the exceptional work required for an A in the Homework component.\n\n\n\n\n\nResearch question : How has the average number of visa-free destinations per passport changed over time in each region (2006‚Äì2025)?",
    "crumbs": [
      "TidyTuesday",
      "<span class='chapter-number'>3</span>¬† <span class='chapter-title'>Homework 02</span>"
    ]
  },
  {
    "objectID": "src/tt/hw03-tt.html",
    "href": "src/tt/hw03-tt.html",
    "title": "Homework 03",
    "section": "",
    "text": "TidyTuesday Section (optional)\nExplore the week‚Äôs TidyTuesday challenge. Develop a research question, then answer it through a short data story with effective visualization(s). Provide sufficient background for readers to grasp your narrative.\nCodeall_recipes &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-16/all_recipes.csv')\ncuisines &lt;- readr::read_csv('https://raw.githubusercontent.com/rfordatascience/tidytuesday/main/data/2025/2025-09-16/cuisines.csv')\nCodelibrary(tidyverse)\nCodecuisines &lt;- cuisines |&gt; \n  mutate(cal_prot = protein * 4,\n         pct_prot = cal_prot / calories * 100) |&gt; \n  filter(!is.na(avg_rating), calories &gt; 0)\n\ntop_cuisine &lt;- cuisines  |&gt; \n  count(country, name = \"n_recipes\")  |&gt; \n  arrange(desc(n_recipes))  |&gt; \n  head(n = 6)  |&gt;  \n  pull(country)\n\ncuisines_top6 &lt;- cuisines |&gt; \n  filter(country %in% top_cuisine)\n\nggplot(cuisines_top6, aes(x = pct_prot, y = avg_rating, color = total_time)) +\n  geom_point(alpha = 0.7) +\n  geom_smooth(method = \"lm\") +\n  facet_wrap(~ country, ncol = 3) +\n  labs(title = \"% Protein & total time needed vs. Average Rating, by Cuisine\",\n       x = \"% of calories from protein\",\n       y = \"Average rating\") +\n  theme_minimal()",
    "crumbs": [
      "TidyTuesday",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Homework 03</span>"
    ]
  },
  {
    "objectID": "src/tt/hw03-tt.html#tidytuesday-section-optional",
    "href": "src/tt/hw03-tt.html#tidytuesday-section-optional",
    "title": "Homework 03",
    "section": "",
    "text": "Instructions\n\n\n\nYou can count work on this week‚Äôs TidyTuesday toward the exceptional work required for an A in the Homework component.\n\n\n\n\nResearch question: How does the percent of calories protein and time needed for the dishes influence average user ratings across different cuisines?",
    "crumbs": [
      "TidyTuesday",
      "<span class='chapter-number'>4</span>¬† <span class='chapter-title'>Homework 03</span>"
    ]
  },
  {
    "objectID": "src/ica/06-wrangling-1-notes.html",
    "href": "src/ica/06-wrangling-1-notes.html",
    "title": "6 Adv Data wrangling P1",
    "section": "",
    "text": "üß© Learning Goals\nBy the end of this lesson, you should be able to:",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>6 Adv Data wrangling P1</span>"
    ]
  },
  {
    "objectID": "src/ica/06-wrangling-1-notes.html#learning-goals",
    "href": "src/ica/06-wrangling-1-notes.html#learning-goals",
    "title": "6 Adv Data wrangling P1",
    "section": "",
    "text": "Determine the class of a given object and identify concerns to be wary of when manipulating an object of that class (numerics, logicals, factors, dates, strings, data.frames)\nExplain what vector recycling is, when it can be a problem, and how to avoid those problems\nUse a variety of functions to wrangle numerical and logical data\nExtract date-time information using the lubridate package\nUse the forcats package to wrangle factor data",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>6 Adv Data wrangling P1</span>"
    ]
  },
  {
    "objectID": "src/ica/06-wrangling-1-notes.html#helpful-cheatsheets",
    "href": "src/ica/06-wrangling-1-notes.html#helpful-cheatsheets",
    "title": "6 Adv Data wrangling P1",
    "section": "Helpful Cheatsheets",
    "text": "Helpful Cheatsheets\nRStudio (Posit) maintains a collection of wonderful cheatsheets. The following will be helpful:\n\nData transformation with dplyr\nDates and times with lubridate\nFactors with forcats\n\nData Wrangling Verbs (from Stat/Comp 112)\n\n\nmutate(): creates/changes columns/elements in a data frame/tibble\n\nselect(): keeps subset of columns/elements in a data frame/tibble\n\nfilter(): keeps subsets of rows in a data frame/tibble\n\narrange(): sorts rows in a data frame/tibble\n\ngroup_by(): internally groups rows in data frame/tibble by values in 1 or more columsn/elements\n\nsummarize(): collapses/combines information across rows using functions such as n(), sum(), mean(), min(), max(), median(), sd()\n\n\ncount(): shortcut for group_by() |&gt; summarize(n = n())\n\n\nleft_join(): mutating join of two data frames/tibbles keeping all rows in left data frame\n\nfull_join(): mutating join of two data frames/tibbles keeping all rows in both data frames\n\ninner_join(): mutating join of two data frames/tibbles keeping rows in left data frame that find match in right\n\nsemi_join(): filtering join of two data frames/tibbles keeping rows in left data frame that find match in right\n\nanti_join(): filtering join of two data frames/tibbles keeping rows in left data frame that do not find match in right\n\npivot_wider(): rearrange values from two columns to many(one column becomes the names of new variables, one column becomes the values of the new variables)\n\npivot_longer(): rearrange values from many columns to two (the names of the columns go to one new variable, the values of the columns go to a second new variable)",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>6 Adv Data wrangling P1</span>"
    ]
  },
  {
    "objectID": "src/ica/06-wrangling-1-notes.html#vectors",
    "href": "src/ica/06-wrangling-1-notes.html#vectors",
    "title": "6 Adv Data wrangling P1",
    "section": "Vectors",
    "text": "Vectors\nAn atomic vector is a storage container in R where all elements in the container are of the same type. The types that are relevant to data science are:\n\n\nlogical (also known as boolean)\nnumbers\n\ninteger\n\nnumeric floating point (also known as double)\n\n\n\ncharacter string\n\nDate and date-time (saved as POSIXct)\nfactor\n\nFunction documentation will refer to vectors frequently.\nSee examples below:\n\n\nggplot2::scale_x_continuous()\n\n\nbreaks: A numeric vector of positions\n\nlabels: A character vector giving labels (must be same length as breaks)\n\n\n\nshiny::sliderInput()\n\n\nvalue: The initial value of the slider [‚Ä¶] A length one vector will create a regular slider; a length two vector will create a double-ended range slider.\n\n\n\nWhen you need a vector, you can create one manually using\n\n\nc(): the combine function\n\nOr you can create one based on available data using\n\n\ndataset |&gt; mutate(newvar = variable &gt; 5) |&gt; pull(newvar): taking one column out of a dataset\n\ndataset |&gt; pull(variable) |&gt; unique(): taking one column out of a dataset and finding unique values\n\n\nCodec(\"Fair\", \"Good\", \"Very Good\", \"Premium\", \"Ideal\")\n\n[1] \"Fair\"      \"Good\"      \"Very Good\" \"Premium\"   \"Ideal\"    \n\nCodediamonds |&gt; pull(cut) |&gt; unique()\n\n[1] Ideal     Premium   Good      Very Good Fair     \nLevels: Fair &lt; Good &lt; Very Good &lt; Premium &lt; Ideal",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>6 Adv Data wrangling P1</span>"
    ]
  },
  {
    "objectID": "src/ica/06-wrangling-1-notes.html#logicals",
    "href": "src/ica/06-wrangling-1-notes.html#logicals",
    "title": "6 Adv Data wrangling P1",
    "section": "Logicals",
    "text": "Logicals\nNotes\nWhat does a logical vector look like?\n\nCodex &lt;- c(TRUE, FALSE, NA)\nx\n\n[1]  TRUE FALSE    NA\n\nCodeclass(x)\n\n[1] \"logical\"\n\n\nYou will often create logical vectors with comparison operators: &gt;, &lt;, &lt;=, &gt;=, ==, !=.\n\nCodex &lt;- c(1, 2, 9, 12)\nx &lt; 2\n\n[1]  TRUE FALSE FALSE FALSE\n\nCodex &lt;= 2\n\n[1]  TRUE  TRUE FALSE FALSE\n\nCodex &gt; 9\n\n[1] FALSE FALSE FALSE  TRUE\n\nCodex &gt;= 9\n\n[1] FALSE FALSE  TRUE  TRUE\n\nCodex == 12\n\n[1] FALSE FALSE FALSE  TRUE\n\nCodex != 12\n\n[1]  TRUE  TRUE  TRUE FALSE\n\n\nWhen you want to check for set containment, the %in% operator is the correct way to do this (as opposed to ==).\n\nCodex &lt;- c(1, 2, 9, 4)\nx == c(1, 2, 4)\n\nWarning in x == c(1, 2, 4): longer object length is not a multiple of shorter\nobject length\n\n\n[1]  TRUE  TRUE FALSE FALSE\n\nCodex %in% c(1, 2, 4)\n\n[1]  TRUE  TRUE FALSE  TRUE\n\n\nThe Warning: longer object length is not a multiple of shorter object length is a manifestation of vector recycling.\nIn R, if two vectors are being combined or compared, the shorter one will be repeated to match the length of the longer one‚Äìeven if longer object length isn‚Äôt a multiple of the shorter object length. We can see the exact recycling that happens below:\n\nCodex &lt;- c(1, 2, 9, 4)\nx == c(1, 2, 4)\n\n[1]  TRUE  TRUE FALSE FALSE\n\nCodex == c(1, 2, 4, 1) # This line demonstrates the recycling that happens on the previous line\n\n[1]  TRUE  TRUE FALSE FALSE\n\n\nLogical vectors can also be created with functions. is.na() is one useful example:\n\nCodex &lt;- c(1, 4, 9, NA)\nx == NA\n\n[1] NA NA NA NA\n\nCodeis.na(x)\n\n[1] FALSE FALSE FALSE  TRUE\n\n\nWe can negate a logical object with !. We can combine logical objects with & (and) and | (or).\n\nCodex &lt;- c(1, 2, 4, 9)\nx &gt; 1 & x &lt; 5\n\n[1] FALSE  TRUE  TRUE FALSE\n\nCode!(x &gt; 1 & x &lt; 5)\n\n[1]  TRUE FALSE FALSE  TRUE\n\nCodex &lt; 2 | x &gt; 8\n\n[1]  TRUE FALSE FALSE  TRUE\n\n\nWe can summarize logical vectors with:\n\n\nany(): Are ANY of the values TRUE?\n\nall(): Are ALL of the values TRUE?\n\nsum(): How many of the values are TRUE?\n\nmean(): What fraction of the values are TRUE?\n\n\nCodex &lt;- c(1, 2, 4, 9)\nany(x == 1)\n\n[1] TRUE\n\nCodeall(x &lt; 10)\n\n[1] TRUE\n\nCodesum(x == 1)\n\n[1] 1\n\nCodemean(x == 1)\n\n[1] 0.25\n\n\nif_else() and case_when() are functions that allow you to return values depending on the value of a logical vector. You‚Äôll explore the documentation for these in the following exercises.\n\n\n\n\n\n\nNote: ifelse() (from base R) and if_else() (from tidyverse) are different functions. We prefer if_else() for many reasons (examples below).\n\nNoisy to make sure you catch issues/bugs\nCan explicitly handle missing values\nKeeps dates as dates\n\n\nExamples\n\nCodex &lt;- c(-1, -2, 4, 9, NA)\n\nifelse(x &gt; 0, 'positive', 'negative')\n\n[1] \"negative\" \"negative\" \"positive\" \"positive\" NA        \n\nCodeif_else(x &gt; 0, 'positive', 'negative')\n\n[1] \"negative\" \"negative\" \"positive\" \"positive\" NA        \n\nCodeifelse(x &gt; 0, 1, 'negative') # Bad: doesn't complain with combo of data types\n\n[1] \"negative\" \"negative\" \"1\"        \"1\"        NA        \n\nCodeif_else(x &gt; 0, 1, 'negative') # Good:noisy to make sure you catch issues\n\nError in `if_else()`:\n! Can't combine `true` &lt;double&gt; and `false` &lt;character&gt;.\n\nCodeif_else(x &gt; 0, 'positive', 'negative', missing = 'missing') # Good: can explicitly handle NA\n\n[1] \"negative\" \"negative\" \"positive\" \"positive\" \"missing\" \n\nCodefun_dates &lt;- mdy('1-1-2025') + 0:365\nifelse(fun_dates &lt; today(), fun_dates + years(), fun_dates) # Bad: converts dates to integers\n\n  [1] 20454 20455 20456 20457 20458 20459 20460 20461 20462 20463 20464 20465\n [13] 20466 20467 20468 20469 20470 20471 20472 20473 20474 20475 20476 20477\n [25] 20478 20479 20480 20481 20482 20483 20484 20485 20486 20487 20488 20489\n [37] 20490 20491 20492 20493 20494 20495 20496 20497 20498 20499 20500 20501\n [49] 20502 20503 20504 20505 20506 20507 20508 20509 20510 20511 20512 20513\n [61] 20514 20515 20516 20517 20518 20519 20520 20521 20522 20523 20524 20525\n [73] 20526 20527 20528 20529 20530 20531 20532 20533 20534 20535 20536 20537\n [85] 20538 20539 20540 20541 20542 20543 20544 20545 20546 20547 20548 20549\n [97] 20550 20551 20552 20553 20554 20555 20556 20557 20558 20559 20560 20561\n[109] 20562 20563 20564 20565 20566 20567 20568 20569 20570 20571 20572 20573\n[121] 20574 20575 20576 20577 20578 20579 20580 20581 20582 20583 20584 20585\n[133] 20586 20587 20588 20589 20590 20591 20592 20593 20594 20595 20596 20597\n[145] 20598 20599 20600 20601 20602 20603 20604 20605 20606 20607 20608 20609\n[157] 20610 20611 20612 20613 20614 20615 20616 20617 20618 20619 20620 20621\n[169] 20622 20623 20624 20625 20626 20627 20628 20629 20630 20631 20632 20633\n[181] 20634 20635 20636 20637 20638 20639 20640 20641 20642 20643 20644 20645\n[193] 20646 20647 20648 20649 20650 20651 20652 20653 20654 20655 20656 20657\n[205] 20658 20659 20660 20661 20662 20663 20664 20665 20666 20667 20668 20669\n[217] 20670 20671 20672 20673 20674 20675 20676 20677 20678 20679 20680 20681\n[229] 20682 20683 20684 20685 20686 20687 20688 20689 20690 20691 20692 20693\n[241] 20694 20695 20696 20697 20698 20699 20700 20701 20702 20703 20704 20705\n[253] 20706 20707 20708 20709 20710 20711 20712 20713 20714 20715 20716 20717\n[265] 20718 20719 20720 20721 20722 20723 20724 20360 20361 20362 20363 20364\n[277] 20365 20366 20367 20368 20369 20370 20371 20372 20373 20374 20375 20376\n[289] 20377 20378 20379 20380 20381 20382 20383 20384 20385 20386 20387 20388\n[301] 20389 20390 20391 20392 20393 20394 20395 20396 20397 20398 20399 20400\n[313] 20401 20402 20403 20404 20405 20406 20407 20408 20409 20410 20411 20412\n[325] 20413 20414 20415 20416 20417 20418 20419 20420 20421 20422 20423 20424\n[337] 20425 20426 20427 20428 20429 20430 20431 20432 20433 20434 20435 20436\n[349] 20437 20438 20439 20440 20441 20442 20443 20444 20445 20446 20447 20448\n[361] 20449 20450 20451 20452 20453 20454\n\nCodeif_else(fun_dates &lt; today(), fun_dates + years(), fun_dates) # Good: keeps dates as dates\n\n  [1] \"2026-01-01\" \"2026-01-02\" \"2026-01-03\" \"2026-01-04\" \"2026-01-05\"\n  [6] \"2026-01-06\" \"2026-01-07\" \"2026-01-08\" \"2026-01-09\" \"2026-01-10\"\n [11] \"2026-01-11\" \"2026-01-12\" \"2026-01-13\" \"2026-01-14\" \"2026-01-15\"\n [16] \"2026-01-16\" \"2026-01-17\" \"2026-01-18\" \"2026-01-19\" \"2026-01-20\"\n [21] \"2026-01-21\" \"2026-01-22\" \"2026-01-23\" \"2026-01-24\" \"2026-01-25\"\n [26] \"2026-01-26\" \"2026-01-27\" \"2026-01-28\" \"2026-01-29\" \"2026-01-30\"\n [31] \"2026-01-31\" \"2026-02-01\" \"2026-02-02\" \"2026-02-03\" \"2026-02-04\"\n [36] \"2026-02-05\" \"2026-02-06\" \"2026-02-07\" \"2026-02-08\" \"2026-02-09\"\n [41] \"2026-02-10\" \"2026-02-11\" \"2026-02-12\" \"2026-02-13\" \"2026-02-14\"\n [46] \"2026-02-15\" \"2026-02-16\" \"2026-02-17\" \"2026-02-18\" \"2026-02-19\"\n [51] \"2026-02-20\" \"2026-02-21\" \"2026-02-22\" \"2026-02-23\" \"2026-02-24\"\n [56] \"2026-02-25\" \"2026-02-26\" \"2026-02-27\" \"2026-02-28\" \"2026-03-01\"\n [61] \"2026-03-02\" \"2026-03-03\" \"2026-03-04\" \"2026-03-05\" \"2026-03-06\"\n [66] \"2026-03-07\" \"2026-03-08\" \"2026-03-09\" \"2026-03-10\" \"2026-03-11\"\n [71] \"2026-03-12\" \"2026-03-13\" \"2026-03-14\" \"2026-03-15\" \"2026-03-16\"\n [76] \"2026-03-17\" \"2026-03-18\" \"2026-03-19\" \"2026-03-20\" \"2026-03-21\"\n [81] \"2026-03-22\" \"2026-03-23\" \"2026-03-24\" \"2026-03-25\" \"2026-03-26\"\n [86] \"2026-03-27\" \"2026-03-28\" \"2026-03-29\" \"2026-03-30\" \"2026-03-31\"\n [91] \"2026-04-01\" \"2026-04-02\" \"2026-04-03\" \"2026-04-04\" \"2026-04-05\"\n [96] \"2026-04-06\" \"2026-04-07\" \"2026-04-08\" \"2026-04-09\" \"2026-04-10\"\n[101] \"2026-04-11\" \"2026-04-12\" \"2026-04-13\" \"2026-04-14\" \"2026-04-15\"\n[106] \"2026-04-16\" \"2026-04-17\" \"2026-04-18\" \"2026-04-19\" \"2026-04-20\"\n[111] \"2026-04-21\" \"2026-04-22\" \"2026-04-23\" \"2026-04-24\" \"2026-04-25\"\n[116] \"2026-04-26\" \"2026-04-27\" \"2026-04-28\" \"2026-04-29\" \"2026-04-30\"\n[121] \"2026-05-01\" \"2026-05-02\" \"2026-05-03\" \"2026-05-04\" \"2026-05-05\"\n[126] \"2026-05-06\" \"2026-05-07\" \"2026-05-08\" \"2026-05-09\" \"2026-05-10\"\n[131] \"2026-05-11\" \"2026-05-12\" \"2026-05-13\" \"2026-05-14\" \"2026-05-15\"\n[136] \"2026-05-16\" \"2026-05-17\" \"2026-05-18\" \"2026-05-19\" \"2026-05-20\"\n[141] \"2026-05-21\" \"2026-05-22\" \"2026-05-23\" \"2026-05-24\" \"2026-05-25\"\n[146] \"2026-05-26\" \"2026-05-27\" \"2026-05-28\" \"2026-05-29\" \"2026-05-30\"\n[151] \"2026-05-31\" \"2026-06-01\" \"2026-06-02\" \"2026-06-03\" \"2026-06-04\"\n[156] \"2026-06-05\" \"2026-06-06\" \"2026-06-07\" \"2026-06-08\" \"2026-06-09\"\n[161] \"2026-06-10\" \"2026-06-11\" \"2026-06-12\" \"2026-06-13\" \"2026-06-14\"\n[166] \"2026-06-15\" \"2026-06-16\" \"2026-06-17\" \"2026-06-18\" \"2026-06-19\"\n[171] \"2026-06-20\" \"2026-06-21\" \"2026-06-22\" \"2026-06-23\" \"2026-06-24\"\n[176] \"2026-06-25\" \"2026-06-26\" \"2026-06-27\" \"2026-06-28\" \"2026-06-29\"\n[181] \"2026-06-30\" \"2026-07-01\" \"2026-07-02\" \"2026-07-03\" \"2026-07-04\"\n[186] \"2026-07-05\" \"2026-07-06\" \"2026-07-07\" \"2026-07-08\" \"2026-07-09\"\n[191] \"2026-07-10\" \"2026-07-11\" \"2026-07-12\" \"2026-07-13\" \"2026-07-14\"\n[196] \"2026-07-15\" \"2026-07-16\" \"2026-07-17\" \"2026-07-18\" \"2026-07-19\"\n[201] \"2026-07-20\" \"2026-07-21\" \"2026-07-22\" \"2026-07-23\" \"2026-07-24\"\n[206] \"2026-07-25\" \"2026-07-26\" \"2026-07-27\" \"2026-07-28\" \"2026-07-29\"\n[211] \"2026-07-30\" \"2026-07-31\" \"2026-08-01\" \"2026-08-02\" \"2026-08-03\"\n[216] \"2026-08-04\" \"2026-08-05\" \"2026-08-06\" \"2026-08-07\" \"2026-08-08\"\n[221] \"2026-08-09\" \"2026-08-10\" \"2026-08-11\" \"2026-08-12\" \"2026-08-13\"\n[226] \"2026-08-14\" \"2026-08-15\" \"2026-08-16\" \"2026-08-17\" \"2026-08-18\"\n[231] \"2026-08-19\" \"2026-08-20\" \"2026-08-21\" \"2026-08-22\" \"2026-08-23\"\n[236] \"2026-08-24\" \"2026-08-25\" \"2026-08-26\" \"2026-08-27\" \"2026-08-28\"\n[241] \"2026-08-29\" \"2026-08-30\" \"2026-08-31\" \"2026-09-01\" \"2026-09-02\"\n[246] \"2026-09-03\" \"2026-09-04\" \"2026-09-05\" \"2026-09-06\" \"2026-09-07\"\n[251] \"2026-09-08\" \"2026-09-09\" \"2026-09-10\" \"2026-09-11\" \"2026-09-12\"\n[256] \"2026-09-13\" \"2026-09-14\" \"2026-09-15\" \"2026-09-16\" \"2026-09-17\"\n[261] \"2026-09-18\" \"2026-09-19\" \"2026-09-20\" \"2026-09-21\" \"2026-09-22\"\n[266] \"2026-09-23\" \"2026-09-24\" \"2026-09-25\" \"2026-09-26\" \"2026-09-27\"\n[271] \"2026-09-28\" \"2025-09-29\" \"2025-09-30\" \"2025-10-01\" \"2025-10-02\"\n[276] \"2025-10-03\" \"2025-10-04\" \"2025-10-05\" \"2025-10-06\" \"2025-10-07\"\n[281] \"2025-10-08\" \"2025-10-09\" \"2025-10-10\" \"2025-10-11\" \"2025-10-12\"\n[286] \"2025-10-13\" \"2025-10-14\" \"2025-10-15\" \"2025-10-16\" \"2025-10-17\"\n[291] \"2025-10-18\" \"2025-10-19\" \"2025-10-20\" \"2025-10-21\" \"2025-10-22\"\n[296] \"2025-10-23\" \"2025-10-24\" \"2025-10-25\" \"2025-10-26\" \"2025-10-27\"\n[301] \"2025-10-28\" \"2025-10-29\" \"2025-10-30\" \"2025-10-31\" \"2025-11-01\"\n[306] \"2025-11-02\" \"2025-11-03\" \"2025-11-04\" \"2025-11-05\" \"2025-11-06\"\n[311] \"2025-11-07\" \"2025-11-08\" \"2025-11-09\" \"2025-11-10\" \"2025-11-11\"\n[316] \"2025-11-12\" \"2025-11-13\" \"2025-11-14\" \"2025-11-15\" \"2025-11-16\"\n[321] \"2025-11-17\" \"2025-11-18\" \"2025-11-19\" \"2025-11-20\" \"2025-11-21\"\n[326] \"2025-11-22\" \"2025-11-23\" \"2025-11-24\" \"2025-11-25\" \"2025-11-26\"\n[331] \"2025-11-27\" \"2025-11-28\" \"2025-11-29\" \"2025-11-30\" \"2025-12-01\"\n[336] \"2025-12-02\" \"2025-12-03\" \"2025-12-04\" \"2025-12-05\" \"2025-12-06\"\n[341] \"2025-12-07\" \"2025-12-08\" \"2025-12-09\" \"2025-12-10\" \"2025-12-11\"\n[346] \"2025-12-12\" \"2025-12-13\" \"2025-12-14\" \"2025-12-15\" \"2025-12-16\"\n[351] \"2025-12-17\" \"2025-12-18\" \"2025-12-19\" \"2025-12-20\" \"2025-12-21\"\n[356] \"2025-12-22\" \"2025-12-23\" \"2025-12-24\" \"2025-12-25\" \"2025-12-26\"\n[361] \"2025-12-27\" \"2025-12-28\" \"2025-12-29\" \"2025-12-30\" \"2025-12-31\"\n[366] \"2026-01-01\"\n\n\n\n\n\n\nExercises\nLoad the diamonds dataset, and filter to the first 1000 diamonds.\n\nCodedata(diamonds)\ndiamonds &lt;- diamonds |&gt; \n    slice_head(n = 1000)\n\n\nUsing tidyverse functions, complete the following:\n\nSubset to diamonds that are less than 400 dollars or more than 10000 dollars.\nSubset to diamonds that are between 500 and 600 dollars (inclusive).\nHow many diamonds are of either Fair, Premium, or Ideal cut (a total count)? What fraction of diamonds are of Fair, Premium, or Ideal cut?\n\nFirst, do this a wrong way with ==. Predict the warning message that you will receive.\nSecond, do this the correct way with an appropriate logical operator.\n\n\nAre there any diamonds of Fair cut that are more than $3000? Are all diamonds of Ideal cut more than $2000?\nCreate two new categorized versions of price by looking up the documentation for if_else() and case_when():\n\n\nprice_cat1: ‚Äúlow‚Äù if price is less than 500 and ‚Äúhigh‚Äù otherwise\n\nprice_cat2: ‚Äúlow‚Äù if price is less than 500, ‚Äúmedium‚Äù if price is between 500 and 1000 dollars inclusive, and ‚Äúhigh‚Äù otherwise.\n\n\n\n\nCode#1 \ndiamonds_extremes &lt;- diamonds |&gt; \n  filter(price &lt; 400 | price &gt; 10000)\n\n#2\ndiamonds_500_600 &lt;- diamonds |&gt; \n  filter(price &gt;= 500 & price &lt;= 600)\n\n#3\nright_count &lt;- diamonds |&gt; \n  filter(cut %in% c(\"Fair\", \"Premium\", \"Ideal\")) |&gt; \n  summarise(count = n(),\n            fraction = n() / nrow(diamonds))\n\n#4\nany_fair_over_3000 &lt;- diamonds  |&gt; \n  filter(cut == \"Fair\")  |&gt; \n  summarise(any_over_3000 = any(price &gt; 3000))\n\nall_ideal_over_2000 &lt;- diamonds  |&gt; \n  filter(cut == \"Ideal\")  |&gt; \n  summarise(all_over_2000 = all(price &gt; 2000))\n\n#5\ndiamonds_with_cats &lt;- diamonds %&gt;%\n  mutate(price_cat1 = if_else(price &lt; 500, \"low\", \"high\"),\n         price_cat2 = case_when(price &lt; 500 ~ \"low\",\n                                price &gt;= 500 & price &lt;= 1000 ~ \"medium\",\n                                price &gt; 1000 ~ \"high\"))",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>6 Adv Data wrangling P1</span>"
    ]
  },
  {
    "objectID": "src/ica/06-wrangling-1-notes.html#numerics",
    "href": "src/ica/06-wrangling-1-notes.html#numerics",
    "title": "6 Adv Data wrangling P1",
    "section": "Numerics",
    "text": "Numerics\nNotes\nNumerical data can be of class integer or numeric (representing real numbers).\n\nCodex &lt;- 1:3\nx\n\n[1] 1 2 3\n\nCodeclass(x)\n\n[1] \"integer\"\n\nCodex &lt;- c(1+1e-9, 2, 3)\nx\n\n[1] 1 2 3\n\nCodeclass(x)\n\n[1] \"numeric\"\n\n\nThe Numbers chapter in R4DS covers the following functions that are all useful for wrangling numeric data:\n\n\nn(), n_distinct(): Counting and counting the number of unique values\n\nsum(is.na()): Counting the number of missing values\n\nmin(), max()\n\n\npmin(), pmax(): Get the min and max across several vectors\nInteger division: %/%. Remainder: %%\n\n\n121 %/% 100 = 1 and 121 %% 100 = 21\n\n\n\n\nround(), floor(), ceiling(): Rounding functions (to a specified number of decimal places, to the largest integer below a number, to the smallest integer above a number)\n\ncut(): Cut a numerical vector into categories\n\ncumsum(), cummean(), cummin(), cummax(): Cumulative functions\n\nrank(): Provide the ranks of the numbers in a vector\n\nlead(), lag(): shift a vector by padding with NAs\nNumerical summaries: mean, median, min, max, quantile, sd, IQR\n\nNote that all numerical summary functions have an na.rm argument that should be set to TRUE if you have missing data.\n\n\nExercises\nExercises will be on HW4.\nThe best way to add these functions and operators to your vocabulary is to need to recall them. Refer to the list of functions above as you try the exercises.\nYou will need to reference function documentation to look at arguments and look in the Examples section.",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>6 Adv Data wrangling P1</span>"
    ]
  },
  {
    "objectID": "src/ica/06-wrangling-1-notes.html#dates",
    "href": "src/ica/06-wrangling-1-notes.html#dates",
    "title": "6 Adv Data wrangling P1",
    "section": "Dates",
    "text": "Dates\nNotes\nThe lubridate package contains useful functions for working with dates and times. The lubridate function reference is a useful resource for finding the functions you need. We‚Äôll take a brief tour of this reference page.\nWe‚Äôll use the lakers dataset in the lubridate package to illustrate some examples.\n\nCodelakers &lt;- as_tibble(lakers)\nhead(lakers)\n\n# A tibble: 6 √ó 13\n     date opponent game_type time  period etype team  player result points type \n    &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;  &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;   &lt;int&gt; &lt;chr&gt;\n1  2.01e7 POR      home      12:00      1 jump‚Ä¶ OFF   \"\"     \"\"          0 \"\"   \n2  2.01e7 POR      home      11:39      1 shot  LAL   \"Pau ‚Ä¶ \"miss‚Ä¶      0 \"hoo‚Ä¶\n3  2.01e7 POR      home      11:37      1 rebo‚Ä¶ LAL   \"Vlad‚Ä¶ \"\"          0 \"off\"\n4  2.01e7 POR      home      11:25      1 shot  LAL   \"Dere‚Ä¶ \"miss‚Ä¶      0 \"lay‚Ä¶\n5  2.01e7 POR      home      11:23      1 rebo‚Ä¶ LAL   \"Pau ‚Ä¶ \"\"          0 \"off\"\n6  2.01e7 POR      home      11:22      1 shot  LAL   \"Pau ‚Ä¶ \"made\"      2 \"hoo‚Ä¶\n# ‚Ñπ 2 more variables: x &lt;int&gt;, y &lt;int&gt;\n\n\nBelow we use date-time parsing functions to represent the date and time variables with date-time classes:\n\nCodelakers &lt;- lakers |&gt;\n    mutate(\n        date = ymd(date),\n        time = ms(time)\n    )\n\n\nBelow we use extraction functions to get components of the date-time objects:\n\nCodelakers_clean &lt;- lakers |&gt;\n    mutate(\n        year = year(date),\n        month = month(date),\n        day = day(date),\n        day_of_week = wday(date, label = TRUE),\n        minute = minute(time),\n        second = second(time)\n    )\nlakers_clean |&gt; select(year:second)\n\n# A tibble: 34,624 √ó 6\n    year month   day day_of_week minute second\n   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;ord&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n 1  2008    10    28 Tue             12      0\n 2  2008    10    28 Tue             11     39\n 3  2008    10    28 Tue             11     37\n 4  2008    10    28 Tue             11     25\n 5  2008    10    28 Tue             11     23\n 6  2008    10    28 Tue             11     22\n 7  2008    10    28 Tue             11     22\n 8  2008    10    28 Tue             11     22\n 9  2008    10    28 Tue             11      0\n10  2008    10    28 Tue             10     53\n# ‚Ñπ 34,614 more rows\n\nCodelakers_clean &lt;- lakers_clean |&gt;\n    group_by(date, opponent, period) |&gt;\n    arrange(date, opponent, period, desc(time)) |&gt;\n    mutate(\n        diff_btw_plays_sec = as.numeric(time - lag(time, 1))\n    )\nlakers_clean |&gt; select(date, opponent, time, period, diff_btw_plays_sec)\n\n# A tibble: 34,624 √ó 5\n# Groups:   date, opponent, period [314]\n   date       opponent time     period diff_btw_plays_sec\n   &lt;date&gt;     &lt;chr&gt;    &lt;Period&gt;  &lt;int&gt;              &lt;dbl&gt;\n 1 2008-10-28 POR      12M 0S        1                 NA\n 2 2008-10-28 POR      11M 39S       1                -21\n 3 2008-10-28 POR      11M 37S       1                 -2\n 4 2008-10-28 POR      11M 25S       1                -12\n 5 2008-10-28 POR      11M 23S       1                 -2\n 6 2008-10-28 POR      11M 22S       1                 -1\n 7 2008-10-28 POR      11M 22S       1                  0\n 8 2008-10-28 POR      11M 22S       1                  0\n 9 2008-10-28 POR      11M 0S        1                -22\n10 2008-10-28 POR      10M 53S       1                 -7\n# ‚Ñπ 34,614 more rows\n\n\nExercises\nExercises will be on HW4.",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>6 Adv Data wrangling P1</span>"
    ]
  },
  {
    "objectID": "src/ica/06-wrangling-1-notes.html#factors",
    "href": "src/ica/06-wrangling-1-notes.html#factors",
    "title": "6 Adv Data wrangling P1",
    "section": "Factors",
    "text": "Factors\nNotes\nCreating factors\nIn R, factors are made up of two components: the actual values of the data and the possible levels within the factor. Creating a factor requires supplying both pieces of information.\n\nCodemonths &lt;- c(\"Mar\", \"Dec\", \"Jan\",  \"Apr\", \"Jul\")\n\n\nHowever, if we were to sort this vector, R would sort this vector alphabetically.\n\nCode# alphabetical sort\nsort(months)\n\n[1] \"Apr\" \"Dec\" \"Jan\" \"Jul\" \"Mar\"\n\n\nWe can fix this sorting by creating a factor version of months. The levels argument is a character vector that specifies the unique values that the factor can take. The order of the values in levels defines the sorting of the factor.\n\nCodemonths_fct &lt;- factor(months, levels = month.abb) # month.abb is a built-in variable\nmonths_fct\n\n[1] Mar Dec Jan Apr Jul\nLevels: Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n\nCodesort(months_fct)\n\n[1] Jan Mar Apr Jul Dec\nLevels: Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n\n\nWhat if we try to create a factor with values that aren‚Äôt in the levels? (e.g., a typo in a month name)\n\nCodemonths2 &lt;- c(\"Jna\", \"Mar\")\nfactor(months2, levels = month.abb)\n\n[1] &lt;NA&gt; Mar \nLevels: Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n\n\nBecause the NA is introduced silently (without any error or warnings), this can be dangerous. It might be better to use the fct() function in the forcats package instead:\n\nCodefct(months2, levels = month.abb)\n\nError in `fct()`:\n! All values of `x` must appear in `levels` or `na`\n‚Ñπ Missing level: \"Jna\"\n\n\nReordering factors\nWe‚Äôll use a subset of the General Social Survey (GSS) dataset available in the forcats pacakges.\n\nCodedata(gss_cat)\nhead(gss_cat)\n\n# A tibble: 6 √ó 9\n   year marital         age race  rincome        partyid     relig denom tvhours\n  &lt;int&gt; &lt;fct&gt;         &lt;int&gt; &lt;fct&gt; &lt;fct&gt;          &lt;fct&gt;       &lt;fct&gt; &lt;fct&gt;   &lt;int&gt;\n1  2000 Never married    26 White $8000 to 9999  Ind,near r‚Ä¶ Prot‚Ä¶ Sout‚Ä¶      12\n2  2000 Divorced         48 White $8000 to 9999  Not str re‚Ä¶ Prot‚Ä¶ Bapt‚Ä¶      NA\n3  2000 Widowed          67 White Not applicable Independent Prot‚Ä¶ No d‚Ä¶       2\n4  2000 Never married    39 White Not applicable Ind,near r‚Ä¶ Orth‚Ä¶ Not ‚Ä¶       4\n5  2000 Divorced         25 White Not applicable Not str de‚Ä¶ None  Not ‚Ä¶       1\n6  2000 Married          25 White $20000 - 24999 Strong dem‚Ä¶ Prot‚Ä¶ Sout‚Ä¶      NA\n\n\nReordering the levels of a factor can be useful in plotting when categories would benefit from being sorted in a particular way:\n\nCoderelig_summary &lt;- gss_cat |&gt;\n    group_by(relig) |&gt;\n    summarize(\n        tvhours = mean(tvhours, na.rm = TRUE),\n        n = n()\n    )\n\nggplot(relig_summary, aes(x = tvhours, y = relig)) + \n    geom_point() +\n    theme_classic()\n\n\n\n\n\n\n\nWe can use fct_reorder() in forcats.\n\nThe first argument is the factor that you want to reorder the levels of\nThe second argument determines how the factor is sorted (analogous to what you put inside arrange() when sorting the rows of a data frame.)\n\n\nCodeggplot(relig_summary, aes(x = tvhours, y = fct_reorder(relig, tvhours))) +\n    geom_point() +\n    theme_classic()\n\n\n\n\n\n\n\nFor bar plots, we can use fct_infreq() to reorder levels from most to least common. This can be combined with fct_rev() to reverse the order (least to most common):\n\nCodegss_cat |&gt;\n    ggplot(aes(x = marital)) +\n    geom_bar() +\n    theme_classic()\n\n\n\n\n\n\nCodegss_cat |&gt;\n    mutate(marital = marital |&gt; fct_infreq() |&gt; fct_rev()) |&gt;\n    ggplot(aes(x = marital)) +\n    geom_bar() +\n    theme_classic()\n\n\n\n\n\n\n\nModifying factor levels\nWe talked about reordering the levels of a factor‚Äìwhat about changing the values of the levels themselves?\nFor example, the names of the political parties in the GSS could use elaboration (‚Äústr‚Äù isn‚Äôt a great label for ‚Äústrong‚Äù) and clean up:\n\nCodegss_cat |&gt; count(partyid)\n\n# A tibble: 10 √ó 2\n   partyid                n\n   &lt;fct&gt;              &lt;int&gt;\n 1 No answer            154\n 2 Don't know             1\n 3 Other party          393\n 4 Strong republican   2314\n 5 Not str republican  3032\n 6 Ind,near rep        1791\n 7 Independent         4119\n 8 Ind,near dem        2499\n 9 Not str democrat    3690\n10 Strong democrat     3490\n\n\nWe can use fct_recode() on partyid with the new level names going on the left and the old levels on the right. Any levels that aren‚Äôt mentioned explicitly (i.e., ‚ÄúDon‚Äôt know‚Äù and ‚ÄúOther party‚Äù) will be left as is:\n\nCodegss_cat |&gt;\n    mutate(\n        partyid = fct_recode(partyid,\n            \"Republican, strong\"    = \"Strong republican\",\n            \"Republican, weak\"      = \"Not str republican\",\n            \"Independent, near rep\" = \"Ind,near rep\",\n            \"Independent, near dem\" = \"Ind,near dem\",\n            \"Democrat, weak\"        = \"Not str democrat\",\n            \"Democrat, strong\"      = \"Strong democrat\"\n        )\n    ) |&gt;\n    count(partyid)\n\n# A tibble: 10 √ó 2\n   partyid                   n\n   &lt;fct&gt;                 &lt;int&gt;\n 1 No answer               154\n 2 Don't know                1\n 3 Other party             393\n 4 Republican, strong     2314\n 5 Republican, weak       3032\n 6 Independent, near rep  1791\n 7 Independent            4119\n 8 Independent, near dem  2499\n 9 Democrat, weak         3690\n10 Democrat, strong       3490\n\n\nTo combine groups, we can assign multiple old levels to the same new level (‚ÄúOther‚Äù maps to ‚ÄúNo answer‚Äù, ‚ÄúDon‚Äôt know‚Äù, and ‚ÄúOther party‚Äù):\n\nCodegss_cat |&gt;\n    mutate(\n        partyid = fct_recode(partyid,\n            \"Republican, strong\"    = \"Strong republican\",\n            \"Republican, weak\"      = \"Not str republican\",\n            \"Independent, near rep\" = \"Ind,near rep\",\n            \"Independent, near dem\" = \"Ind,near dem\",\n            \"Democrat, weak\"        = \"Not str democrat\",\n            \"Democrat, strong\"      = \"Strong democrat\",\n            \"Other\"                 = \"No answer\",\n            \"Other\"                 = \"Don't know\",\n            \"Other\"                 = \"Other party\"\n        )\n    )\n\n# A tibble: 21,483 √ó 9\n    year marital         age race  rincome        partyid    relig denom tvhours\n   &lt;int&gt; &lt;fct&gt;         &lt;int&gt; &lt;fct&gt; &lt;fct&gt;          &lt;fct&gt;      &lt;fct&gt; &lt;fct&gt;   &lt;int&gt;\n 1  2000 Never married    26 White $8000 to 9999  Independe‚Ä¶ Prot‚Ä¶ Sout‚Ä¶      12\n 2  2000 Divorced         48 White $8000 to 9999  Republica‚Ä¶ Prot‚Ä¶ Bapt‚Ä¶      NA\n 3  2000 Widowed          67 White Not applicable Independe‚Ä¶ Prot‚Ä¶ No d‚Ä¶       2\n 4  2000 Never married    39 White Not applicable Independe‚Ä¶ Orth‚Ä¶ Not ‚Ä¶       4\n 5  2000 Divorced         25 White Not applicable Democrat,‚Ä¶ None  Not ‚Ä¶       1\n 6  2000 Married          25 White $20000 - 24999 Democrat,‚Ä¶ Prot‚Ä¶ Sout‚Ä¶      NA\n 7  2000 Never married    36 White $25000 or more Republica‚Ä¶ Chri‚Ä¶ Not ‚Ä¶       3\n 8  2000 Divorced         44 White $7000 to 7999  Independe‚Ä¶ Prot‚Ä¶ Luth‚Ä¶      NA\n 9  2000 Married          44 White $25000 or more Democrat,‚Ä¶ Prot‚Ä¶ Other       0\n10  2000 Married          47 White $25000 or more Republica‚Ä¶ Prot‚Ä¶ Sout‚Ä¶       3\n# ‚Ñπ 21,473 more rows\n\n\nWe can use fct_collapse() to collapse many levels:\n\nCodegss_cat |&gt;\n    mutate(\n        partyid = fct_collapse(partyid,\n            \"Other\" = c(\"No answer\", \"Don't know\", \"Other party\"),\n            \"Republican\" = c(\"Strong republican\", \"Not str republican\"),\n            \"Independent\" = c(\"Ind,near rep\", \"Independent\", \"Ind,near dem\"),\n            \"Democrat\" = c(\"Not str democrat\", \"Strong democrat\")\n        )\n    ) |&gt;\n    count(partyid)\n\n# A tibble: 4 √ó 2\n  partyid         n\n  &lt;fct&gt;       &lt;int&gt;\n1 Other         548\n2 Republican   5346\n3 Independent  8409\n4 Democrat     7180\n\n\nExercises\n\nCreate a factor version of the following data with the levels in a sensible order.\n\n\nCoderatings &lt;- c(\"High\", \"Medium\", \"Low\")\n\n\nMore exercises will be on HW4.",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>6 Adv Data wrangling P1</span>"
    ]
  },
  {
    "objectID": "src/ica/06-wrangling-1-notes.html#done",
    "href": "src/ica/06-wrangling-1-notes.html#done",
    "title": "6 Adv Data wrangling P1",
    "section": "Done!",
    "text": "Done!\n\nCheck the ICA Instructions for how to (a) push your code to GitHub and (b) update your portfolio website",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>5</span>¬† <span class='chapter-title'>6 Adv Data wrangling P1</span>"
    ]
  },
  {
    "objectID": "src/ica/08-missing-data-notes.html",
    "href": "src/ica/08-missing-data-notes.html",
    "title": "8 Missing Data",
    "section": "",
    "text": "üß© Learning Goals\nBy the end of this lesson, you should be able to:",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>8 Missing Data</span>"
    ]
  },
  {
    "objectID": "src/ica/08-missing-data-notes.html#learning-goals",
    "href": "src/ica/08-missing-data-notes.html#learning-goals",
    "title": "8 Missing Data",
    "section": "",
    "text": "Go through a data quality checklist when data wrangling\nExplain the difference between MCAR, MAR, and MNAR missing data mechanisms\nAssess what missing data mechanisms might be at play in a given dataset\nUse visualizations to explore missing data patterns\nExplain why multiple imputation is preferred to single imputation\nExplain how a simulation study can be used to investigate properties of statistical methods",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>8 Missing Data</span>"
    ]
  },
  {
    "objectID": "src/ica/08-missing-data-notes.html#data-quality-checklist",
    "href": "src/ica/08-missing-data-notes.html#data-quality-checklist",
    "title": "8 Missing Data",
    "section": "Data Quality Checklist",
    "text": "Data Quality Checklist\nWhen wrangling / cleaning data, make sure to check the assumptions you make about the data to ensure you don‚Äôt lose data quality.\n\nData Parsing (reading data into a different data format)\n\nAlways keep the original, raw data (don‚Äôt manually change it).\nUse Test Cases: Find rows or write test cases to double check the wrangling works as expected\nDATES: When using lubridate to parse dates and times, ensure the strings are consistently ordered and formatted correctly, eg, mm/dd/yy vs.¬†dd/mm/yy.\nSTRINGS: When using stringr to parse strings with regular expressions, check example rows to ensure that the pattern captured all of the examples you want and excluded the patterns you don‚Äôt want.\nAlways check for missing values to see if the missing ones are expected given the original data.\n\n\nData Joining\n\n\nIdentify missing values in the key variables and decide how to handle them before the merge process (e.g., omitting rows with missing values, imputing missing values).\nDecide on the correct join type (left, right, inner, full, etc.) OR if the data structure is the same use list_rbind() to bind rows or list_cbind() to bind columns.\nIf doing a join, make sure that the key variables (by) have the same meaning in both datasets and are represented in the same way (e.g., id = 1 to 20 in first dataset will match id = 1 - 20 in undesirable ways)\nPredict the number of rows that will result from the join and double check the anti_join() to see which rows did not find a match.\nCheck for duplicate records within each dataset and ensure they are handled appropriately before merging.\nVerify that the merged dataset maintains consistency with the original datasets in terms of data values, variable names, and variable types.\nPerform some preliminary analysis or validation checks on the merged dataset to ensure that it meets the requirements of your analysis.\n\n\nSanity Check: Visualize your data!!!\n\n\nDo the right number of points appear?\nDo the values seem reasonable?",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>8 Missing Data</span>"
    ]
  },
  {
    "objectID": "src/ica/08-missing-data-notes.html#explicit-v.-implicit-missing-data",
    "href": "src/ica/08-missing-data-notes.html#explicit-v.-implicit-missing-data",
    "title": "8 Missing Data",
    "section": "Explicit v. Implicit Missing Data",
    "text": "Explicit v. Implicit Missing Data\nExplicit missing data is data that is explicitly marked as missing. In R, this is done with NA.\nImplicit missing data is data that is missing but not explicitly marked as such.\n\nThis can happen when an entire row is missing from a dataset.\n\nFor example, if a study participant doesn‚Äôt attend a follow-up visit. It maybe not even be recorded in the data.\n\n\n\nWe need to make implicit missingness explicit before we can work with the data.\n\nConsider the combinations of variables that you‚Äôd expect to be fully present in a complete dataset.\n\nIf a combination is missing, you can create a new row with explicit missing values.\n\nFor example, if you expect every participant (each has a unique pid) to have an observation for each visit, you could use the function complete() to create that new row and plug in values of NA for the missing data.\n\n\n\n\n\nstudy_data_full &lt;- study_data |&gt; \n  complete(pid, visit)",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>8 Missing Data</span>"
    ]
  },
  {
    "objectID": "src/ica/08-missing-data-notes.html#dealing-with-missing-data",
    "href": "src/ica/08-missing-data-notes.html#dealing-with-missing-data",
    "title": "8 Missing Data",
    "section": "Dealing with missing data",
    "text": "Dealing with missing data\nIf you have explicit missing data, there are 3 main ways of proceeding:\n\n\nDrop the cases or rows with any missing data from the analysis‚Äìa complete case analysis\n\nPro: Easy to implement\nCon: reduces sample size, introduces bias if the missing data is not ‚Äúmissing completely at random‚Äù\n\n\nCreate a category for missing values.\n\nExample: The survey question ‚ÄúWhat is your gender?‚Äù might only provide two possible responses: ‚Äúmale‚Äù and ‚Äúfemale‚Äù. Missing values for this could indicate that the respondent uses a non-binary designation. Instead of dropping these cases, treating the missing data as its own category (‚ÄúDoes not wish to answer‚Äù) would be more appropriate.\nPros: Maintains sample size, may help us if data is ‚Äúmissing not at random‚Äù\nCons: Not directly applicable to continuous outcomes (could include interactions with a categorical version in models to account for it)\n\n\n\nImpute (or fill in values for) the missing data using imputation algorithms.\n\nImputation algorithms can be as simple as replacing missing values with the mean of the non-missing values (very simplistic).\nRegression imputation algorithms use models to predict missing values as a function of other variables in the data.\nPros: Maintains sample size, multiple regression imputation minimizes bias if ‚Äúmissing at random‚Äù\nCons: Computationally intensive\n\n\n\n\nDeciding between these options and proceeding with choosing finer details within an option requires an understanding of the mechanism by which data become missing.",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>8 Missing Data</span>"
    ]
  },
  {
    "objectID": "src/ica/08-missing-data-notes.html#missing-data-mechanisms",
    "href": "src/ica/08-missing-data-notes.html#missing-data-mechanisms",
    "title": "8 Missing Data",
    "section": "Missing Data Mechanisms",
    "text": "Missing Data Mechanisms\nThe reasons for which a variable might have missing data are divided into 3 mechanisms: MCAR, MAR, and MNAR.\nWithin a dataset, multiple mechanisms may be present‚Äìwe need to consider the missingness mechanism for each variable individually.\n\n\n\nMissing completely at random (MCAR):\n\nThe probability of missing data for a variable is the same for all cases. Implies that causes of the missing data are unrelated to the data. (https://stefvanbuuren.name/fimd/sec-MCAR.html)\nExamples:\n\nMeasurement device that runs out of batteries causes the remainder of observations for the day to be missing.\nData entry software requires a particular field to be typo-free, and missing values are introduced when there are typos.\n\n\nImplications for downstream work:\n\nIf a variable has MCAR missingness, a complete case analysis will be unbiased (still valid).\nHowever, with a lot of missing observations, a complete case analysis will suffer from loss of statistical power (ability to detect a real difference), and imputation will be useful to retain the original sample size.\n\n\n\n\n\nMissing at random (MAR):\n\nThe probability of missing data is related to observed variables but unrelated to unobserved information.\nExamples:\n\nBlood pressure measurements tend to be missing in patients in worse health. (Those in worse health are more likely to miss clinic visits.) Better and worse health can be measured by a variety of indicators in their health record.\nIn a survey, older people are more likely to report their income than younger people. Missingness is related to the observed age variable, but not to unobserved information.\n\n\nImplications for downstream work:\n\nTry to use imputation methods that predict the value of the missing variables from other observed variables. Assessing whether this can be done accurately takes some exploration‚Äìwe‚Äôll explore this shortly.\n\n\n\n\n\nMissing not at random (MNAR):\n\nThe probability of missing data is related to unobserved variables (and probably observed variables too).\nExamples:\n\nBlood pressure measurements are more likely to be missing for those with the highest blood pressure. This is MNAR rather than MAR because the missing data on blood pressure is related to the unobserved values themselves.\nHigh-income individuals may be less likely to report their income.\n\n\nImplications for downstream work:\n\nIdeally, we would learn more about the causes for the missingness. This could allow us to use more informed imputation models.\n\nExample: Biological measurements that tend to be missing because of concentrations that are too low (a phenomenon known as left-censoring). Imputation methods specifically suited to left-censoring are useful here.\n\n\nWe can use imputation methods with different assumptions about the missing data and try out a variety of assumptions. This lets us see how sensitive the results are under various scenarios.\n\nExample: If higher incomes are more likely to be missing, we can make different assumptions about what ‚Äúhigh‚Äù could be to fill in the missing values and see how our results change under these different assumptions.",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>8 Missing Data</span>"
    ]
  },
  {
    "objectID": "src/ica/08-missing-data-notes.html#exercise",
    "href": "src/ica/08-missing-data-notes.html#exercise",
    "title": "8 Missing Data",
    "section": "Exercise",
    "text": "Exercise\n\n\nMissing data mechanism For each of the following situations, propose what missing data mechanism you think is most likely at play.\n\n\nIn a clinical trial, some patients dropped out before the end of the study. Their reasons for dropping out were not recorded.\nA weather station records temperature, humidity, and wind speed every hour. Some of the recorded values are missing.\nA social media platform collects data on user interactions, such as likes, comments, and shares. Some interactions are not recorded due to bugs in the code.",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>8 Missing Data</span>"
    ]
  },
  {
    "objectID": "src/ica/08-missing-data-notes.html#exploring-missing-data",
    "href": "src/ica/08-missing-data-notes.html#exploring-missing-data",
    "title": "8 Missing Data",
    "section": "Exploring Missing Data",
    "text": "Exploring Missing Data\nGuiding question: How can we use visualizations and tabulations to explore what missing data mechanisms may be at play?\nWe‚Äôll look at the airquality dataset available in base R, which gives daily air quality measurements in New York from May to September 1973. You can pull up the codebook with ?airquality in the Console.\n\nCodedata(airquality)",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>8 Missing Data</span>"
    ]
  },
  {
    "objectID": "src/ica/08-missing-data-notes.html#missingness-by-variable",
    "href": "src/ica/08-missing-data-notes.html#missingness-by-variable",
    "title": "8 Missing Data",
    "section": "Missingness by Variable",
    "text": "Missingness by Variable\nWe can explore how much missingness there is for each variable with the following functions:\n\nCodesummary(airquality) # Summary statistics in addition to number of NA's\n\n     Ozone           Solar.R           Wind             Temp      \n Min.   :  1.00   Min.   :  7.0   Min.   : 1.700   Min.   :56.00  \n 1st Qu.: 18.00   1st Qu.:115.8   1st Qu.: 7.400   1st Qu.:72.00  \n Median : 31.50   Median :205.0   Median : 9.700   Median :79.00  \n Mean   : 42.13   Mean   :185.9   Mean   : 9.958   Mean   :77.88  \n 3rd Qu.: 63.25   3rd Qu.:258.8   3rd Qu.:11.500   3rd Qu.:85.00  \n Max.   :168.00   Max.   :334.0   Max.   :20.700   Max.   :97.00  \n NA's   :37       NA's   :7                                       \n     Month            Day      \n Min.   :5.000   Min.   : 1.0  \n 1st Qu.:6.000   1st Qu.: 8.0  \n Median :7.000   Median :16.0  \n Mean   :6.993   Mean   :15.8  \n 3rd Qu.:8.000   3rd Qu.:23.0  \n Max.   :9.000   Max.   :31.0  \n                               \n\nCodenaniar::vis_miss(airquality) # Where are NA's located?\n\n\n\n\n\n\nCodenaniar::miss_var_summary(airquality) # Information from vis_miss() in table form\n\n# A tibble: 6 √ó 3\n  variable n_miss pct_miss\n  &lt;chr&gt;     &lt;int&gt;    &lt;num&gt;\n1 Ozone        37    24.2 \n2 Solar.R       7     4.58\n3 Wind          0     0   \n4 Temp          0     0   \n5 Month         0     0   \n6 Day           0     0",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>8 Missing Data</span>"
    ]
  },
  {
    "objectID": "src/ica/08-missing-data-notes.html#missingness-by-case",
    "href": "src/ica/08-missing-data-notes.html#missingness-by-case",
    "title": "8 Missing Data",
    "section": "Missingness by Case",
    "text": "Missingness by Case\nWe can explore how much missingness there is for each case with naniar::miss_case_summary(). For each case, this function calculates the number and percentage of variables with a missing value.\nImpact of Information: If the pct_miss column is large for a case, we likely won‚Äôt be able to impute any of its missing values because there just isn‚Äôt enough known information‚Äìthis case will have to be dropped from the analysis.\n\nCodenaniar::miss_case_summary(airquality)\n\n# A tibble: 153 √ó 3\n    case n_miss pct_miss\n   &lt;int&gt;  &lt;int&gt;    &lt;dbl&gt;\n 1     5      2     33.3\n 2    27      2     33.3\n 3     6      1     16.7\n 4    10      1     16.7\n 5    11      1     16.7\n 6    25      1     16.7\n 7    26      1     16.7\n 8    32      1     16.7\n 9    33      1     16.7\n10    34      1     16.7\n# ‚Ñπ 143 more rows",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>8 Missing Data</span>"
    ]
  },
  {
    "objectID": "src/ica/08-missing-data-notes.html#exploring-missingness-mechanisms",
    "href": "src/ica/08-missing-data-notes.html#exploring-missingness-mechanisms",
    "title": "8 Missing Data",
    "section": "Exploring Missingness Mechanisms",
    "text": "Exploring Missingness Mechanisms\nAssessing missingness mechanisms involves checking if missingness in a variable is related to other variables.\nNote: Through our available data, we are really only able to explore the potential for MCAR or MAR mechanisms.\nImpact of Information: There is always the chance that unobserved information (unobserved other variables or unobserved values of the variables we do have) is related to missingness for our variables, so to think through the potential for MNAR, more contextual information is necessary.\nTo explore these relationships, we can create TRUE/FALSE indicators of whether a variable is missing. In the plots below, we use is.na(Ozone) to explore whether cases with missing ozone values are noticeably different from cases with observed ozone values in terms of Solar.R.\n\nCodeggplot(airquality, aes(x = is.na(Ozone), y = Solar.R)) + \n    geom_boxplot()\n\n\n\n\n\n\nCodeggplot(airquality, aes(x = Solar.R, color = is.na(Ozone))) + \n    geom_density()\n\n\n\n\n\n\n\nThe above boxplots and density plots suggest that missing ozone is not strongly related to solar radiation levels.\nWe still should check if ozone missingness is related to the Wind, Temp, Month, and Day variables (to be done in Exercises).\nIn addition to checking if the chance of ozone missingness is related to Solar.R, we should check if the values of ozone could be predicted by Solar.R.\nIn the scaterrplot below, we look at the relationship between Ozone and Solar.R and use vertical lines to indicate the Solar.R values for cases that are missing Ozone.\n\n\nImpact of Information: We see that missing Ozone cases are within the observed span of Solar.R, so we would be ok with predicting Ozone from Solar.R because there would be no extrapolation.\n\n\nCodeggplot(airquality, aes(x = Solar.R, y = Ozone)) +\n    geom_point() +\n    geom_smooth() +\n    geom_vline(data = airquality |&gt; filter(is.na(Ozone)), mapping = aes(xintercept = Solar.R))",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>8 Missing Data</span>"
    ]
  },
  {
    "objectID": "src/ica/08-missing-data-notes.html#exercises",
    "href": "src/ica/08-missing-data-notes.html#exercises",
    "title": "8 Missing Data",
    "section": "Exercises",
    "text": "Exercises\n\n\nMechanism detection practice Look at the boxplot + scatterplot pairs for Alternate Situations 1 and 2 below. How do these situations compare to our actual situation and to each other? What concerns might arise from using a model to impute Ozone?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOzone mechanism detection Continue the investigation of missingness for Ozone.\n\nWe want to see how Month, Wind, and Temp relate to the chance of missingness for Ozone and to the value of Ozone.\nDoes it look like a linear regression model (perhaps with variable transformations) could be effective in imputing the missing ozone data?\n\nCodeairquality_mod  |&gt; \n  mutate(is_na = is.na(Ozone))  |&gt; \n  group_by(is_na)  |&gt; \n  summarize(mean_Wind = mean(Wind, na.rm=TRUE),\n            mean_Temp = mean(Temp, na.rm=TRUE),\n            prop_Month5 = mean(Month==5))\n\n# A tibble: 2 √ó 4\n  is_na mean_Wind mean_Temp prop_Month5\n  &lt;lgl&gt;     &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;\n1 FALSE      9.86      77.9       0.224\n2 TRUE      10.3       77.9       0.135\n\nCodeairquality_mod  |&gt; \n  group_by(Month)  |&gt; \n  summarize(missing_rate = mean(is.na(Ozone)))  |&gt; \n  ggplot(aes(Month, missing_rate)) +\n    geom_col()\n\n\n\n\n\n\nCodeairquality_mod  |&gt; \n  ggplot(aes(factor(is.na(Ozone)), Wind)) +\n    geom_boxplot() +\n    scale_x_discrete(labels=c(\"Observed\",\"Missing\")) +\n    ylab(\"Wind (mph)\")\n\n\n\n\n\n\nCodeairquality_mod  |&gt; \n  ggplot(aes(factor(is.na(Ozone)), Temp)) +\n    geom_boxplot() +\n    scale_x_discrete(labels=c(\"Observed\",\"Missing\")) +\n    ylab(\"Temp (¬∞F)\")",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>8 Missing Data</span>"
    ]
  },
  {
    "objectID": "src/ica/08-missing-data-notes.html#regression-imputation",
    "href": "src/ica/08-missing-data-notes.html#regression-imputation",
    "title": "8 Missing Data",
    "section": "Regression Imputation",
    "text": "Regression Imputation\nWhen a model is built and used to generate a single set of predictions for missing values, this is known as single imputation.\n\nWhen using singly imputed data in subsequent modeling, the uncertainty in estimates tends to be underestimated. This means that:\n\nStandard errors are lower than they should be.\nConfidence intervals won‚Äôt contain the true parameter value the ‚Äúadvertised‚Äù percentage of times\n\ne.g., 95% confidence intervals will not contain the truth in 95% of samples‚Äìthe coverage probability will be less than 95%\n\n\n\n\n\nIn multiple imputation, multiple imputed datasets are generated with different values for the filled-in data.\n\nSubsequent models are fit on each of these datasets, and both estimates and uncertainty measures are pooled across all of these fits.\nMultiple imputation more accurately estimates uncertainty measures.",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>8 Missing Data</span>"
    ]
  },
  {
    "objectID": "src/ica/08-missing-data-notes.html#simulation-studies",
    "href": "src/ica/08-missing-data-notes.html#simulation-studies",
    "title": "8 Missing Data",
    "section": "Simulation Studies",
    "text": "Simulation Studies\nWe can use a simulation study to investigate the statistical properties described above.\n\n\nGenerate (simulate) data where we are in control of missing data mechanisms and the true relationship between an outcome and the predictors.\nOn that simulated data, use single imputation to fill in the missing values. Fit the desired model, and obtain a confidence interval for a coefficient of interest.\nOn that simulated data, use multiple imputation to fill in the missing values. Fit the desired models on all imputed datasets, pool results, and obtain a confidence interval for a coefficient of interest.\nSteps 1 - 3 are repeated a lot of times (num_simulations &lt;- 1000) to see how things work out in lots of different samples.\nSummarize the performance of single and multiple imputation across the num_simulations simulations.\n\n\nWe will slowly step through the simulation study code below. We will pause frequently for you to add comments documenting what is happening.\n\nCode# set.seed(224)\n# num_simulations &lt;- 1000\n# ci_list &lt;- vector(\"list\", length = num_simulations)\n# \n# system.time({\n# for (i in 1:num_simulations) {\n#     # Simulate data\n#     n &lt;- 1000\n#     sim_data &lt;- tibble(\n#         x1 = runif(n, min = 0, max = 1),\n#         x2 = x1 + rnorm(n, mean = 0, sd = 1),\n#         x2_miss_bool = rbinom(n, size = 1, prob = x1/2),\n#         x2_NA = if_else(x2_miss_bool == 1, NA, x2),\n#         y = x1 + x2 + rnorm(n, mean = 0, sd = 1)\n#     )\n#     \n#     # Single imputation ---------------\n#     mice_obj &lt;- mice(sim_data |&gt; select(x1, x2_NA, y), m = 1, method = \"norm\", printFlag = FALSE)\n#     si_mod &lt;- with(mice_obj, lm(y ~ x1 + x2_NA))\n#     ci_single &lt;- si_mod$analyses[[1]] |&gt; confint(level = 0.95)\n#     ci_single &lt;- ci_single[\"x2_NA\",]\n#     \n#     # Multiple imputation -------------\n#     mice_obj &lt;- mice(sim_data |&gt; select(x1, x2_NA, y), m = 10, method = \"norm\", printFlag = FALSE)\n#     mi_mods &lt;- with(mice_obj, lm(y ~ x1 + x2_NA))\n#     pooled_res &lt;- pool(mi_mods)\n#     summ_pooled_res &lt;- summary(pooled_res, conf.int = TRUE, conf.level = 0.95)\n#     ci_multiple_lower &lt;- summ_pooled_res |&gt; filter(term==\"x2_NA\") |&gt; pull(`2.5 %`)\n#     ci_multiple_upper &lt;- summ_pooled_res |&gt; filter(term==\"x2_NA\") |&gt; pull(`97.5 %`)\n#     \n#     # Store CI information\n#     ci_list[[i]] &lt;- tibble(\n#         ci_lower = c(\n#             ci_single[1],\n#             ci_multiple_lower\n#         ),\n#         ci_upper = c(\n#             ci_single[2],\n#             ci_multiple_upper\n#         ),\n#         which_imp = c(\"single\", \"multiple\")\n#     )\n# }\n# })\n# ```\n# \n# Below we compute the confidence interval (CI) coverage probability (fraction of times the CI contains the true value of `1`) for the CIs generated from single and multiple imputation:\n# \n# ```{r mult_single_sim_study_summ}\n# ci_data &lt;- bind_rows(ci_list)\n# ci_data |&gt; \n#     mutate(contains_truth = ci_lower &lt; 1 & ci_upper &gt; 1) |&gt; \n#     group_by(which_imp) |&gt; \n#     summarize(frac_contains_truth = mean(contains_truth))",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>8 Missing Data</span>"
    ]
  },
  {
    "objectID": "src/ica/08-missing-data-notes.html#done",
    "href": "src/ica/08-missing-data-notes.html#done",
    "title": "8 Missing Data",
    "section": "Done!",
    "text": "Done!\n\nCheck the ICA Instructions for how to (a) push your code to GitHub and (b) update your portfolio website",
    "crumbs": [
      "In-Class Activities",
      "<span class='chapter-number'>6</span>¬† <span class='chapter-title'>8 Missing Data</span>"
    ]
  },
  {
    "objectID": "mm/mm.html",
    "href": "mm/mm.html",
    "title": "Appendix A ‚Äî Mind Maps",
    "section": "",
    "text": "Creativity",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Mind Maps</span>"
    ]
  },
  {
    "objectID": "mm/mm.html#creativity",
    "href": "mm/mm.html#creativity",
    "title": "Appendix A ‚Äî Mind Maps",
    "section": "",
    "text": "0808-mind_map_example.jpg",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Mind Maps</span>"
    ]
  },
  {
    "objectID": "mm/mm.html#review",
    "href": "mm/mm.html#review",
    "title": "Appendix A ‚Äî Mind Maps",
    "section": "Review",
    "text": "Review\n\n\n\n0905-mind_map.jpg",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Mind Maps</span>"
    ]
  },
  {
    "objectID": "mm/mm.html#adv-data-viz",
    "href": "mm/mm.html#adv-data-viz",
    "title": "Appendix A ‚Äî Mind Maps",
    "section": "Adv Data Viz",
    "text": "Adv Data Viz\n\n\n\n0908-mind_map.jpg",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Mind Maps</span>"
    ]
  },
  {
    "objectID": "mm/mm.html#adv-spatial-viz",
    "href": "mm/mm.html#adv-spatial-viz",
    "title": "Appendix A ‚Äî Mind Maps",
    "section": "Adv Spatial Viz",
    "text": "Adv Spatial Viz\n\n\n\n0910-mind_map.jpg",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Mind Maps</span>"
    ]
  },
  {
    "objectID": "mm/mm.html#adv-data-wrangling-p1",
    "href": "mm/mm.html#adv-data-wrangling-p1",
    "title": "Appendix A ‚Äî Mind Maps",
    "section": "Adv Data wrangling P1",
    "text": "Adv Data wrangling P1\n\n\n\n0917-mind_map.jpg",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Mind Maps</span>"
    ]
  },
  {
    "objectID": "mm/mm.html#adv-data-wrangling-p2",
    "href": "mm/mm.html#adv-data-wrangling-p2",
    "title": "Appendix A ‚Äî Mind Maps",
    "section": "Adv Data wrangling P2",
    "text": "Adv Data wrangling P2\n\n\n\n0922-mind_map.jpg",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Mind Maps</span>"
    ]
  },
  {
    "objectID": "mm/mm.html#missing-data",
    "href": "mm/mm.html#missing-data",
    "title": "Appendix A ‚Äî Mind Maps",
    "section": "Missing Data",
    "text": "Missing Data\n\n\n\n0929-mind_map.jpg",
    "crumbs": [
      "Appendices",
      "<span class='chapter-number'>A</span>¬† <span class='chapter-title'>Mind Maps</span>"
    ]
  }
]